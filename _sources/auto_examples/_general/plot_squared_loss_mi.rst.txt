
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/_general/plot_squared_loss_mi.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples__general_plot_squared_loss_mi.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples__general_plot_squared_loss_mi.py:


=================================================================
Clustering with the squared-loss mutual information
=================================================================

The squared-loss mutual information (SMI) Is a variant of mutual
information proposed in [1]_

In this variant, the Pearson divergence is considered as
replacement for the KL divergence. The resulting cost function
can be used with any clustering architecture.

We show in this example how to combine this loss,
:class:`gemclus.gemini.ChiSquareGEMINI` with a kernel logistic
regression.

.. [1] Sugiyama, M., Yamada, M., Kimura, M., & Hachiya, H. (2011).
        On information-maximization clustering: Tuning parameter
        selection and analytic solution. ICML 2011.

.. GENERATED FROM PYTHON SOURCE LINES 21-57



.. image-sg:: /auto_examples/_general/images/sphx_glr_plot_squared_loss_mi_001.png
   :alt: plot squared loss mi
   :srcset: /auto_examples/_general/images/sphx_glr_plot_squared_loss_mi_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    1.0






|

.. code-block:: Python


    from gemclus.linear import LinearModel
    from gemclus.gemini import ChiSquareGEMINI
    from sklearn import datasets, metrics
    from sklearn.metrics import pairwise
    from matplotlib import pyplot as plt
    import numpy as np

    # Create the dataset
    X, y = datasets.make_circles(n_samples=200, factor=0.1, noise=0.05, random_state=0)
    # Center data
    X = (X-X.mean(0))/X.std(0)

    # Compute the kernel
    kernel = pairwise.pairwise_kernels(X, metric="rbf")

    # Create the linear model
    model = LinearModel(n_clusters=2,  gemini=ChiSquareGEMINI(), random_state=0)

    model.fit(kernel) # Linear regression on kernel = kernel model

    y_pred = model.predict(kernel)
    print(metrics.adjusted_rand_score(y, y_pred))

    # we can also use generalisation to visualise the decision boundary

    x_vals = np.linspace(-3, 3, num=50)
    y_vals = np.linspace(-3, 3, num=50)
    xx, yy = np.meshgrid(x_vals, y_vals)
    grid_inputs = np.c_[xx.ravel(), yy.ravel()]
    kernelised_grid_inputs = pairwise.pairwise_kernels(grid_inputs, X, metric="rbf")
    zz = model.predict(kernelised_grid_inputs).reshape((50, 50))

    # Plot decision boundary with predictions on top
    plt.contourf(xx, yy, zz, alpha=0.5, cmap="Blues")
    plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap="Reds_r")
    plt.show()

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.418 seconds)


.. _sphx_glr_download_auto_examples__general_plot_squared_loss_mi.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_squared_loss_mi.ipynb <plot_squared_loss_mi.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_squared_loss_mi.py <plot_squared_loss_mi.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_squared_loss_mi.zip <plot_squared_loss_mi.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
