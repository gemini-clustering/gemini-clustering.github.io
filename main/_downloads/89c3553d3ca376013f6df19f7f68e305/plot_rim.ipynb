{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Simple logistic regression with RIM\n\nRIM (regularised mutual information) is a proposal of model by Krause et al. (2010) which consists in\nmaximising for a linear model under $\\ell_2$ penalty. In this example, we show how to do clustering of a\nGaussian mixture using RIM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn import datasets\n\nfrom gemclus.linear import RIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a simple synthetic dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate samples on that are simple to separate\nX, y = datasets.make_blobs(centers=3, cluster_std=0.5, n_samples=200, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\nCreate the RIM clustering model (just a logistic regression) and fit it to the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = RIM(n_clusters=3, random_state=0)\nclf.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Clustering\nLet us take a look at the decision boundaries according to the probabilities\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Predict a grad of inputs\nx_vals = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, num=50)\ny_vals = np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, num=50)\nxx, yy = np.meshgrid(x_vals, y_vals)\ngrid_inputs = np.c_[xx.ravel(), yy.ravel()]\ngrid_pred = clf.predict_proba(grid_inputs)\n\n# Isolate probability of the argmax\nzz = grid_pred.max(1)\nzz = zz.reshape((50, 50))\n\nplt.contourf(xx, yy, zz, alpha=0.3, levels=10)\n\n# Now, show the cluster predictions\ny_pred = clf.predict(X)\nX_0 = X[y_pred == 0]\nX_1 = X[y_pred == 1]\nX_2 = X[y_pred == 2]\n\nax0 = plt.scatter(X_0[:, 0], X_0[:, 1], c='crimson', s=50)\nax1 = plt.scatter(X_1[:, 0], X_1[:, 1], c='deepskyblue', s=50)\nax2 = plt.scatter(X_2[:, 0], X_2[:, 1], c='darkgreen', s=50)\n\nleg = plt.legend([ax0, ax1, ax2],\n                 ['Cluster 0', 'Cluster 1', 'Cluster 2'],\n                 loc='upper left', fancybox=True, scatterpoints=1)\nleg.get_frame().set_alpha(0.5)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.show()\n\nprint(clf.score(X))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}