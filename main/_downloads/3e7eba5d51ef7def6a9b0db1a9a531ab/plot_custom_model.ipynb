{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Extending GemClus to build your own discriminative clustering model\n\nIt is possible that the model we wish to use is not available among the choices of GemClus. To that end, it is\npossible to extend the framework and define custom models that can still be trained by GEMINI.\n\nIn this example, we define a simple bias-less logistic regression for 2 classes with a sigmoid activation function\nand train it to tell apart two isotropic Gaussian distributions.\n\nThe model can be written as:\n\n\\begin{align}p_\\theta(y=1|x) = \\text{sigmoid}(x^\\top \\theta)\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom gemclus import DiscriminativeModel\nfrom gemclus.data import draw_gmm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the custom model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class BinaryRegression(DiscriminativeModel):\n    # We start by defining the same parameters as expected by the parent class for Discriminative model\n    def __init__(self, n_clusters=3, gemini=\"mi\", max_iter=1000, learning_rate=1e-3, solver=\"adam\", batch_size=None,\n                 verbose=False, random_state=None):\n        super().__init__(\n            n_clusters=n_clusters,\n            gemini=gemini,\n            max_iter=max_iter,\n            learning_rate=learning_rate,\n            solver=solver,\n            batch_size=batch_size,\n            verbose=verbose,\n            random_state=random_state\n        )\n\n    def _init_params(self, random_state, X=None):\n        # We define the initialisation of our parameters using the random state\n        in_threshold = np.sqrt(1 / X.shape[1])\n        self.theta_ = random_state.uniform(-in_threshold, in_threshold, size=(X.shape[1], 1))\n\n    def _get_weights(self):\n        # For the optimiser, all parameters that can be optimised need to be returned inside a list\n        return [self.theta_]\n\n    def _infer(self, X, retain=True):\n        # This function computes the output of the model. You must ensure to have a probability vector\n        y_pred_logit = np.matmul(X, self.theta_)\n\n        # Compute sigmoid of prediction\n        y_pred = 1 / (1.0 + np.exp(-y_pred_logit))\n\n        if retain:\n            # The retain flag here means that we are allowed to store intermediate information\n            # For example, information useful for backpropagation\n            self._y_sigmoid = y_pred\n\n        # As the GEMINI objectives expect arrays to be of shape (n_samples, n_clusters), we duplicate\n        # the opposite probability on the column axis\n        y_pred = np.concatenate([y_pred.reshape((-1, 1)), (1 - y_pred).reshape((-1, 1))], axis=1)\n\n        # It is important that the columns add up to 1 for each row, otherwise the GEMINI will not work\n\n        return y_pred\n\n    def _compute_grads(self, X, y_pred, gradient):\n        # The gradient has a shape of (n_samples, n_clusters) following the prediction we used in `_infer`\n\n        # We first start our gradient by correcting the column axis extension\n        gradient = gradient[:, 0] - gradient[:, 1]\n\n        # We apply the gradient of the sigmoid\n        gradient = self._y_sigmoid * (1 - self._y_sigmoid) * gradient.reshape((-1, 1))\n\n        # And we finish with the gradient on our model parameter\n        theta_grad = np.matmul(X.T, gradient)\n\n        # We return this gradient inside a list. The order of gradients must be matching the order of the parameters\n        # in `_get_weights`\n\n        # As the goal of GEMINI is to maximise contrary to common framework which aim for minimisation,\n        # we return the negative of the gradient\n        return [-theta_grad]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the fitting procedure and plot the clustering results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can test this model\nX, y = draw_gmm(n=100, loc=np.eye(2), scale=[np.eye(2) * 0.2, np.eye(2) * 0.2], pvals=np.ones(2) / 2, random_state=0)\n\ncustom_model = BinaryRegression(gemini=\"wasserstein_ovo\", n_clusters=2)\n\ny_pred = custom_model.fit_predict(X)\n\nplt.title(\"Clustering using a custom unsupervised binary regression model\")\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}