{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Example of decision boundary map for a mixture of Gaussian and low-degree Student distributions\n\nThis example is a retake from the experiment in the original GEMINI paper where we want to find the true clusters\nin a mixture of Gaussian that incorporates a low-degree-of-freedom student t-distribution. Consequently, this\ndistribution generates sample that may seem like outliers if we are to expect only Gaussian distribution.\n\nUnlike the paper, this example here is done with the `gemclus.linear.LinearWasserstein` instead of an MLP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nfrom gemclus.linear import LinearWasserstein\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the data\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taking 200 samples, 1 degree of freedom and not-so-far apart means\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N=200\nalpha = 3\ndf = 1\n\n# Generate first the multivariate Gaussian distribution\nnp.random.seed(0)\n\npy = np.ones(4) / 4\nmeans = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]]) * alpha\n\ny = np.random.multinomial(1, py, size=N).argmax(1)\nproportions = np.bincount(y)\ny.sort()\n\ncovariance = np.eye(2)\n\nX = []\nfor k in range(3):\n    X += [np.random.multivariate_normal(means[k], covariance, size=proportions[k])]\n\n# Sample from the student t distribution\nnx = np.random.multivariate_normal(np.zeros(2), covariance, size=proportions[-1])\nu = np.random.chisquare(df, proportions[-1]).reshape((-1, 1))\nx = np.sqrt(df / u) * nx + np.expand_dims(means[-1], axis=0)\nX += [x]\n\nX = np.concatenate(X, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model for clustering\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = LinearWasserstein(n_clusters=4, random_state=0)\ny_pred = clf.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Clustering\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now, generate as well grid inputs to help drawing the decision boundary\nx_vals = np.linspace(-10,10,num=50)\ny_vals = np.linspace(-10,10,num=50)\nxx,yy=np.meshgrid(x_vals,y_vals)\ngrid_inputs = np.c_[xx.ravel(),yy.ravel()]\nzz = clf.predict(grid_inputs).reshape((50,50))\n\n# Plot decision boundary with predictions on top\nplt.contourf(xx,yy,zz,alpha=0.5,cmap=\"Blues\")\nplt.scatter(X[:,0],X[:,1],c=y_pred,cmap=\"Reds_r\")\n\nplt.xlim(-10,10)\nplt.ylim(-10,10)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}