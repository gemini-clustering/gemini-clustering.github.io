{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Feature selection using the Sparse MMD OvA (MLP)\n\nIn this example, we ask the :class:`gemclus.sparse.SparseMLPMMD` to perform a path where the regularisation penalty is\nprogressively increased until all features but 2 are discarded. The model then keeps the best weights with the\nminimum number of features that maintains a GEMINI score close to 90% of the maximum GEMINI value encountered during\nthe path.\n\nThe dataset consists of 3 isotropic Gaussian distributions (so 3 clusters to find) in 2d with 48 noisy variables. Thus,\nthe optimal solution should find that only 2 features are relevant and sufficient to get the correct clustering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn import datasets\n\nfrom gemclus.sparse import SparseMLPMMD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a simple synthetic dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate samples on that are simple to separate\nX, y = datasets.make_blobs(centers=3, cluster_std=0.5, n_samples=200, random_state=0)\n\n# Add extra noisy variables\nnp.random.seed(0)\nX = np.concatenate([X, np.random.normal(scale=0.5, size=(200, 48))], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\nCreate the GEMINI clustering model (just a logistic regression) and call the .path method to iteratively select\nfeatures through gradient descent.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = SparseMLPMMD(random_state=0, alpha=1)\n\n# Perform a path search to eliminate all features\nbest_weights, geminis, penalties, alphas, n_features = clf.path(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Path results\n\nTake a look at how our features are distributed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Highlight the number of selected features and the GEMINI along decreasing increasing alphas\nplt.figure(figsize=(10, 5))\nplt.subplot(2, 2, 1)\nplt.title(\"Number of features depending on $\\\\alpha$\")\nplt.plot(alphas, n_features)\nplt.xlabel(\"Regularisation penalty $\\\\alpha$\")\nplt.ylabel(\"Number of used features\")\nplt.subplot(2, 2, 2)\nplt.title(\"GEMINI score depending on $\\\\alpha$\")\nplt.plot(alphas, geminis)\nplt.xlabel(\"$\\\\alpha$\")\nplt.ylabel(\"GEMINI score\")\nplt.ylim(0, max(geminis) + 0.5)\nplt.subplot(2, 2, 3)\nplt.title(\"Group-Lasso penalty depending on $\\\\alpha$\")\nplt.plot(alphas, penalties)\nplt.xlabel(\"$\\\\alpha$\")\nplt.ylabel(\"Penalty\")\nplt.subplot(2, 2, 4)\nplt.title(\"Total score depending on $\\\\alpha$\")\nplt.plot(alphas, np.array(geminis) - np.array(penalties) * alphas)\nplt.xlabel(\"$\\\\alpha$\")\nplt.ylabel(\"Total score\")\nplt.tight_layout()\nplt.show()\n\nprint(f\"Selected features: {np.where(np.linalg.norm(best_weights[0], axis=1, ord=2) != 0)}\")\nprint(f\"The model score is {clf.score(X)}\")\nprint(f\"Top gemini score was {max(geminis)}, which settles an optimum of {0.9 * max(geminis)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Clustering\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now, show the cluster predictions\ny_pred = clf.predict(X)\nX_0 = X[y_pred == 0]\nX_1 = X[y_pred == 1]\nX_2 = X[y_pred == 2]\n\nax0 = plt.scatter(X_0[:, 0], X_0[:, 1], c='crimson', s=50)\nax1 = plt.scatter(X_1[:, 0], X_1[:, 1], c='deepskyblue', s=50)\nax2 = plt.scatter(X_2[:, 0], X_2[:, 1], c='darkgreen', s=50)\n\nleg = plt.legend([ax0, ax1, ax2],\n                 ['Cluster 0', 'Cluster 1', 'Cluster 2'],\n                 loc='upper left', fancybox=True, scatterpoints=1)\nleg.get_frame().set_alpha(0.5)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}